import gradio as gr
from PIL import Image
import os


def get_images(path):
    images = [Image.open(os.path.join(path, im)) for im in os.listdir(path)]
    paths = os.listdir(path)
    return [(im, path) for im, path in zip(images, paths)]


with gr.Blocks() as demo:
    gr.Markdown(
        """
                ## Stable Bias:  Analyzing Societal Representations in Diffusion Models
            """
    )
    gr.HTML(
        """
     <p style="margin-bottom: 10px; font-size: 94%">This is the demo page for the "Stable Bias" paper, which aims to explore and quantify social biases in text-to-image systems. <br> This work was done by <a href='https://huggingface.co/sasha' style='text-decoration: underline;' target='_blank'> Alexandra Sasha Luccioni (Hugging Face) </a>, <a href='https://huggingface.co/cakiki' style='text-decoration: underline;' target='_blank'> Christopher Akiki (ScaDS.AI, Leipzig University)</a>, <a href='https://huggingface.co/meg' style='text-decoration: underline;' target='_blank'> Margaret Mitchell (Hugging Face) </a> and  <a href='https://huggingface.co/yjernite' style='text-decoration: underline;' target='_blank'> Yacine Jernite (Hugging Face) </a> .</p>
          """
    )
    examples_path = "images/examples"
    examples_gallery = gr.Gallery(
        get_images(examples_path),
        label="Example images",
        show_label=False,
        elem_id="gallery",
    ).style(grid=[1, 6], height="auto")
    gr.HTML(
        """
     <p style="margin-bottom: 10px; font-size: 94%"> Example images generated by three text-to-image models (Dall-E 2, Stable Diffusion v1.4 and v.2). </p>
          """
    )
    gr.HTML(
        """
     <p style="margin-bottom: 14px; font-size: 100%">  As AI-enabled Text-to-Image systems are becoming increasingly used, characterizing the social biases they exhibit is a necessary first step to lowering their risk of discriminatory outcomes. <br> We propose a new method for exploring and quantifying social biases in these kinds of systems by directly comparing collections of generated images designed to showcase a system’s variation across social attributes — gender and ethnicity — and target attributes for bias evaluation — professions and gender-coded adjectives. <br> We compare three models: Stable Diffusion v.1.4, Stable Diffusion v.2., and Dall-E 2 and find <b> clear evidence of ethnicity and gender biases </b>. <br> You can explore these findings in the collapsed sections below, which present our findings: </p>
          """
    )

    with gr.Accordion("Identity group results (ethnicity and gender)", open=False):
        gr.HTML(
            """
         <p style="margin-bottom: 14px; font-size: 100%">  One of the approaches that we adopted in our work is hierarchical clustering of the images generated by the text-to-image systems in response to prompts that include identity terms with regards to ethnicity and gender. <br> We computed 3 different numbers of clusters (12, 24 and 48) and created an <a href='https://huggingface.co/spaces/society-ethics/DiffusionFaceClustering' style='text-decoration: underline;' target='_blank'> Identity Representation Demo </a> that allows for the exploration of the different clusters and their contents. </p>
              """
        )
        with gr.Row():
            impath = "images/identities"
            identity_gallery = gr.Gallery(
                [os.path.join(impath, im) for im in os.listdir(impath)],
                label="Identity cluster images",
                show_label=False,
                elem_id="gallery",
            ).style(grid=3, height="auto")
        gr.HTML(
            """
         <p style="margin-bottom: 14px; font-size: 100%"> TO DO: talk about what we see above. <br> Continue exploring the demo on your own to uncover other patterns!  </p>
              """
        )

    with gr.Accordion("Bias Exploration", open=False):
        gr.HTML(
            """
         <p style="margin-bottom: 14px; font-size: 100%">  We queried our 3 systems with prompts that included names of professions, and one of our goals was to explore the social biases of these models. <br> Since artificial depictions of fictive
humans have no inherent gender or ethnicity nor do they belong to socially-constructed groups, we pursued our analysis <b> without </b> ascribing gender and ethnicity categories to the images generated. <b> We do this by calculating the correlations between the professions and the different identity clusters that we identified. <br> Using both the <a href='https://huggingface.co/spaces/society-ethics/DiffusionClustering' style='text-decoration: underline;' target='_blank'> Diffusion Cluster Explorer </a> and the <a href='https://huggingface.co/spaces/society-ethics/DiffusionFaceClustering' style='text-decoration: underline;' target='_blank'> Identity Representation Demo </a>, we can see which clusters are most correlated with each profession and what identities are in these clusters.</p>
              """
        )
        with gr.Row():
            gr.HTML(
                """
         <p style="margin-bottom: 14px; font-size: 100%">  Using the <a href='https://huggingface.co/spaces/society-ethics/DiffusionClustering' style='text-decoration: underline;' target='_blank'> Diffusion Cluster Explorer </a>, we can see that the top cluster for the CEO and director professions is Cluster 4: </p> """
            )
            ceo_img = gr.Image(
                Image.open("images/bias/ceo_dir.png"),
                label="CEO Image",
                show_label=False,
            )

        with gr.Row():
            gr.HTML(
                """
         <p style="margin-bottom: 14px; font-size: 100%"> Going back to the <a href='https://huggingface.co/spaces/society-ethics/DiffusionFaceClustering' style='text-decoration: underline;' target='_blank'> Identity Representation Demo </a>, we can see that the most represented gender term is man (56%  of the cluster) and White (29% of the cluster).  </p> """
            )
            cluster4 = gr.Image(
                Image.open("images/bias/Cluster4.png"),
                label="Cluster 4 Image",
                show_label=False,
            )
        with gr.Row():
            gr.HTML(
                """
         <p style="margin-bottom: 14px; font-size: 100%">  If we look at the cluster representation of professions such as social assistant and social worker, we can observe that the former is best represented by Cluster 2, whereas the latter has a more uniform representation across multiple clusters:   </p> """
            )
            social_img = gr.Image(
                Image.open("images/bias/social.png"),
                label="social image",
                show_label=False,
            )

        with gr.Row():
            gr.HTML(
                """
         <p style="margin-bottom: 14px; font-size: 100%"> Cluster 2 is best represented by the gender term is woman (81%) as well as Latinx (19%). </p> """
            )
            cluster4 = gr.Image(
                Image.open("images/bias/Cluster2.png"),
                label="Cluster 2 Image",
                show_label=False,
            )
        with gr.Row():
            gr.HTML(
                """
             <p style="margin-bottom: 14px; font-size: 100%"> TO DO: talk about what we see above. <br> Continue exploring the demo on your own to uncover other patterns!  </p>"""
            )

    with gr.Accordion("Comparing model generations", open=False):
        gr.HTML(
            """
         <p style="margin-bottom: 14px; font-size: 100%">  One of the goals of our study was allowing users to compare model generations across professions in an open-ended way, uncovering patterns and trends on their own. This is why we created the <a href='https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer' style='text-decoration: underline;' target='_blank'> Diffusion Bias Explorer </a> and the <a href='https://huggingface.co/spaces/society-ethics/Average_diffusion_faces' style='text-decoration: underline;' target='_blank'> Average Diffusion Faces </a> tools. <br> We show some of their functionalities below:    </p> """
        )
        with gr.Row():
            with gr.Column():
                impath = "images/biasexplorer"
                biasexplorer_gallery = gr.Gallery(
                    [os.path.join(impath, im) for im in os.listdir(impath)],
                    label="Bias explorer images",
                    show_label=False,
                    elem_id="gallery",
                ).style(grid=2, height="auto")
            with gr.Column():
                gr.HTML(
                    """
                 <p style="margin-bottom: 14px; font-size: 100%"> Comparing generations both between two models and within a single model can help uncover trends and patterns that are hard to measure using quantitative approaches. </p>"""
                )
        with gr.Row():
            impath = "images/averagefaces"
            average_gallery = gr.Gallery(
                [os.path.join(impath, im) for im in os.listdir(impath)],
                label="Average Face images",
                show_label=False,
                elem_id="gallery",
            ).style(grid=3, height="auto")
            gr.HTML(
                """
                 <p style="margin-bottom: 14px; font-size: 100%"> Looking at the average faces for a given profession across multiple models can help see the dominant characteristics of that profession, as well as how much variation there is (based on how fuzzy the image is). </p>"""
            )

    with gr.Accordion("Exploring the color space of generated images", open=False):
        gr.HTML(
            """
         <p style="margin-bottom: 14px; font-size: 100%">  TODO Chris   </p> """
        )

    with gr.Accordion(
        "Exploring the nearest neighbors of generated images", open=False
    ):
        gr.HTML(
            """
         <p style="margin-bottom: 14px; font-size: 100%">  TODO Chris   </p> """
        )

    gr.Markdown(
        """
                ### All of the tools created as part of this project:
            """
    )
    gr.HTML(
        """
     <p style="margin-bottom: 10px; font-size: 94%">
     <a href='https://huggingface.co/spaces/society-ethics/Average_diffusion_faces' style='text-decoration: underline;' target='_blank'> Average Diffusion Faces </a>  <br>
     <a href='https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer' style='text-decoration: underline;' target='_blank'> Diffusion Bias Explorer </a> <br> 
     <a href='https://huggingface.co/spaces/society-ethics/DiffusionClustering' style='text-decoration: underline;' target='_blank'> Diffusion Cluster Explorer </a>
     <a href='https://huggingface.co/spaces/society-ethics/DiffusionFaceClustering' style='text-decoration: underline;' target='_blank'> Identity Representation Demo </a>
     <a href='https://huggingface.co/spaces/tti-bias/identities-bovw-knn' style='text-decoration: underline;' target='_blank'> BoVW Nearest Neighbors Explorer </a> <br> 
     <a href='https://huggingface.co/spaces/tti-bias/professions-bovw-knn' style='text-decoration: underline;' target='_blank'> BoVW Professions Explorer </a> <br>    
     <a href='https://huggingface.co/spaces/tti-bias/identities-colorfulness-knn' style='text-decoration: underline;' target='_blank'> Colorfulness Profession Explorer </a> <br>      
     <a href='https://huggingface.co/spaces/tti-bias/professions-colorfulness-knn' style='text-decoration: underline;' target='_blank'> Colorfulness Identities Explorer </a> <br> </p>
          """
    )
    gr.Interface.load("spaces/pngwn/DiffusionBiasExplorer")

if __name__ == "__main__":
    demo.launch()
