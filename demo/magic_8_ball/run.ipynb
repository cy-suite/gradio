{"cells": [{"cell_type": "markdown", "id": "302934307671667531413257853548643485645", "metadata": {}, "source": ["# Gradio Demo: magic_8_ball"]}, {"cell_type": "code", "execution_count": null, "id": "272996653310673477252411125948039410165", "metadata": {}, "outputs": [], "source": ["!pip install -q gradio https://gradio-builds.s3.amazonaws.com/bed454c3d22cfacedc047eb3b0ba987b485ac3fd/gradio-4.40.0-py3-none-any.whl git+https://github.com/huggingface/parler-tts.git accelerate"]}, {"cell_type": "code", "execution_count": null, "id": "288918539441861185822528903084949547379", "metadata": {}, "outputs": [], "source": ["# Downloading files from the demo repo\n", "import os\n", "!wget -q https://github.com/gradio-app/gradio/raw/main/demo/magic_8_ball/streamer.py"]}, {"cell_type": "code", "execution_count": null, "id": "44380577570523278879349135829904343037", "metadata": {}, "outputs": [], "source": ["import io\n", "from threading import Thread\n", "import random\n", "import os\n", "\n", "import numpy as np\n", "import spaces\n", "import gradio as gr\n", "import torch\n", "\n", "from parler_tts import ParlerTTSForConditionalGeneration\n", "from pydub import AudioSegment\n", "from transformers import AutoTokenizer, AutoFeatureExtractor, set_seed\n", "from huggingface_hub import InferenceClient\n", "from streamer import ParlerTTSStreamer\n", "import time\n", "\n", "\n", "device = (\n", "    \"cuda:0\"\n", "    if torch.cuda.is_available()\n", "    else \"mps\"\n", "    if torch.backends.mps.is_available()\n", "    else \"cpu\"\n", ")\n", "torch_dtype = torch.float16 if device != \"cpu\" else torch.float32\n", "\n", "repo_id = \"parler-tts/parler_tts_mini_v0.1\"\n", "\n", "jenny_repo_id = \"ylacombe/parler-tts-mini-jenny-30H\"\n", "\n", "model = ParlerTTSForConditionalGeneration.from_pretrained(\n", "    jenny_repo_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n", ").to(device)\n", "\n", "client = InferenceClient(token=os.getenv(\"HF_TOKEN\"))\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n", "feature_extractor = AutoFeatureExtractor.from_pretrained(repo_id)\n", "\n", "SAMPLE_RATE = feature_extractor.sampling_rate\n", "SEED = 42\n", "\n", "\n", "def numpy_to_mp3(audio_array, sampling_rate):\n", "    # Normalize audio_array if it's floating-point\n", "    if np.issubdtype(audio_array.dtype, np.floating):\n", "        max_val = np.max(np.abs(audio_array))\n", "        audio_array = (audio_array / max_val) * 32767  # Normalize to 16-bit range\n", "        audio_array = audio_array.astype(np.int16)\n", "\n", "    # Create an audio segment from the numpy array\n", "    audio_segment = AudioSegment(\n", "        audio_array.tobytes(),\n", "        frame_rate=sampling_rate,\n", "        sample_width=audio_array.dtype.itemsize,\n", "        channels=1,\n", "    )\n", "\n", "    # Export the audio segment to MP3 bytes - use a high bitrate to maximise quality\n", "    mp3_io = io.BytesIO()\n", "    audio_segment.export(mp3_io, format=\"mp3\", bitrate=\"320k\")\n", "\n", "    # Get the MP3 bytes\n", "    mp3_bytes = mp3_io.getvalue()\n", "    mp3_io.close()\n", "\n", "    return mp3_bytes\n", "\n", "\n", "sampling_rate = model.audio_encoder.config.sampling_rate\n", "frame_rate = model.audio_encoder.config.frame_rate\n", "\n", "\n", "def generate_response(audio):\n", "    gr.Info(\"Transcribing Audio\", duration=5)\n", "\n", "    asr_result = client.automatic_speech_recognition(audio)\n", "    question = asr_result if isinstance(asr_result, str) else str(asr_result)\n", "\n", "    messages = [\n", "        {\"role\": \"system\", \"content\": (\"You are a magic 8 ball. ...\")},\n", "        {\n", "            \"role\": \"user\",\n", "            \"content\": f\"Magic 8 Ball please answer this question -  {question}\",\n", "        },\n", "    ]\n", "\n", "    response = client.text_generation(\n", "        model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n", "        prompt=str(messages),\n", "        max_new_tokens=64,\n", "        temperature=0.7,\n", "        seed=random.randint(1, 5000),\n", "    )\n", "\n", "    if isinstance(response, str):\n", "        response_text = response\n", "    elif hasattr(response, \"generated_text\"):\n", "        response_text = response.generated_text\n", "    else:\n", "        response_text = str(response)\n", "\n", "    response_text = response_text.replace(\"Magic 8 Ball\", \"\")\n", "    return response_text, None, None\n", "\n", "\n", "@spaces.GPU\n", "def read_response(answer):\n", "    play_steps_in_s = 2.0\n", "    play_steps = int(frame_rate * play_steps_in_s)\n", "\n", "    description = \"Jenny speaks at an average pace with a calm delivery in a very confined sounding environment with clear audio quality.\"\n", "    description_tokens = tokenizer(description, return_tensors=\"pt\").to(device)\n", "\n", "    streamer = ParlerTTSStreamer(model, device=device, play_steps=play_steps)\n", "    prompt = tokenizer(answer, return_tensors=\"pt\").to(device)\n", "\n", "    generation_kwargs = {\n", "        \"input_ids\": description_tokens.input_ids,\n", "        \"prompt_input_ids\": prompt.input_ids,\n", "        \"streamer\": streamer,\n", "        \"do_sample\": True,\n", "        \"temperature\": 1.0,\n", "        \"min_new_tokens\": 10,\n", "    }\n", "\n", "    set_seed(SEED)\n", "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n", "    thread.start()\n", "    start = time.time()\n", "    for new_audio in streamer:\n", "        print(\n", "            f\"Sample of length: {round(new_audio.shape[0] / sampling_rate, 2)} seconds after {time.time() - start} seconds\"\n", "        )\n", "        yield answer, numpy_to_mp3(new_audio, sampling_rate=sampling_rate)\n", "\n", "\n", "with gr.Blocks() as demo:\n", "    gr.HTML(\n", "        \"\"\"\n", "        <h1 style='text-align: center;'> Magic 8 Ball \ud83c\udfb1 </h1>\n", "        <h3 style='text-align: center;'> Ask a question and receive wisdom </h3>\n", "        <p style='text-align: center;'> Powered by <a href=\"https://github.com/huggingface/parler-tts\"> Parler-TTS</a>\n", "        \"\"\"\n", "    )\n", "    with gr.Group():\n", "        with gr.Row():\n", "            audio_out = gr.Audio(\n", "                label=\"Spoken Answer\", streaming=True, autoplay=True, loop=False\n", "            )\n", "            answer = gr.Textbox(label=\"Answer\")\n", "            state = gr.State()\n", "        with gr.Row():\n", "            gr.Markdown(\n", "                \"Example questions: 'Should I get a dog?', 'What is the meaning of life?'\"\n", "            )\n", "            audio_in = gr.Audio(\n", "                label=\"Speak you question\", sources=\"microphone\", type=\"filepath\"\n", "            )\n", "    with gr.Row():\n", "        gr.HTML(\n", "            \"\"\"<h3 style='text-align: center;'> Examples: 'What is the meaning of life?', 'Should I get a dog?' </h3>\"\"\"\n", "        )\n", "    audio_in.stop_recording(\n", "        generate_response, audio_in, [state, answer, audio_out]\n", "    ).then(fn=read_response, inputs=state, outputs=[answer, audio_out])\n", "\n", "\n", "if __name__ == \"__main__\":\n", "    demo.launch()\n"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}