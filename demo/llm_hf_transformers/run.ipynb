{"cells": [{"cell_type": "markdown", "id": "302934307671667531413257853548643485645", "metadata": {}, "source": ["# Gradio Demo: llm_hf_transformers"]}, {"cell_type": "code", "execution_count": null, "id": "272996653310673477252411125948039410165", "metadata": {}, "outputs": [], "source": ["!pip install -q gradio transformers>=4.46.0 torch>=2.3.1 "]}, {"cell_type": "code", "execution_count": null, "id": "288918539441861185822528903084949547379", "metadata": {}, "outputs": [], "source": ["import gradio as gr\n", "import torch\n", "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n", "from threading import Thread\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\n", "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", torch_dtype=torch.float16)\n", "model = model.to('cuda:0')\n", "\n", "class StopOnTokens(StoppingCriteria):\n", "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n", "        stop_ids = [29, 0]\n", "        return any(input_ids[0][-1] == stop_id for stop_id in stop_ids)\n", "\n", "def predict(message, history):\n", "    history_transformer_format = list(zip(history[:-1], history[1:])) + [[message, \"\"]]\n", "    stop = StopOnTokens()\n", "\n", "    messages = \"\".join([\"\".join([\"\\n<human>:\"+item[0], \"\\n<bot>:\"+item[1]])\n", "                for item in history_transformer_format])\n", "\n", "    model_inputs = tokenizer([messages], return_tensors=\"pt\").to(\"cuda\")\n", "    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n", "    generate_kwargs = dict(\n", "        model_inputs,\n", "        streamer=streamer,\n", "        max_new_tokens=1024,\n", "        do_sample=True,\n", "        top_p=0.95,\n", "        top_k=1000,\n", "        temperature=1.0,\n", "        num_beams=1,\n", "        stopping_criteria=StoppingCriteriaList([stop])\n", "        )\n", "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n", "    t.start()\n", "\n", "    partial_message = \"\"\n", "    for new_token in streamer:\n", "        if new_token != '<':\n", "            partial_message += new_token\n", "            yield partial_message\n", "\n", "demo = gr.ChatInterface(predict, type=\"messages\")\n", "\n", "if __name__ == \"__main__\":\n", "    demo.launch()\n"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}